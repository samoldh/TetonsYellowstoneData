{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y31OwpktMmpy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"YellowstoneLakesData-cleaned v1.xlsx\"\n",
        "output_name = \"YellowstoneLakesData-cleaned.csv\"\n",
        "national_park = \"Yellowstone\""
      ],
      "metadata": {
        "id": "fWovK5Debp1O"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in Excel file\n",
        "dictionary = pd.read_excel(filepath, sheet_name=None)\n",
        "\n",
        "# Iterate through each DataFrame\n",
        "for sheet_name, df in dictionary.items():\n",
        "  # check if the first two columns are empty, then if they are drop them\n",
        "  if df.iloc[:, 0].isnull().all() and df.iloc[:, 1].isnull().all():\n",
        "    df = df.drop(df.columns[[0, 1]], axis=1)\n",
        "    df = df.reset_index(drop=True)\n",
        "  elif df.iloc[:, 0].isnull().all():\n",
        "    df = df.drop(df.columns[[0]], axis=1)\n",
        "    df = df.reset_index(drop=True)\n",
        "  elif df.iloc[:, 1].isnull().all():\n",
        "    df = df.drop(df.columns[[1]], axis=1)\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "  # Read down each column until the first value ignoring any that include the string \"this table\", then use that value as the column title.\n",
        "  for column_index in range(df.shape[1]):\n",
        "    for row_index in range(df.shape[0]):\n",
        "      if not pd.isna(df.iloc[row_index, column_index]) and \"this table\" not in str(df.iloc[row_index, column_index]):\n",
        "        df.columns.values[column_index] = df.iloc[row_index, column_index]\n",
        "        # If the row below the first value is not blank add that value to the column title\n",
        "        if not pd.isna(df.iloc[row_index+1, column_index]) and not df.iloc[row_index, column_index] == \"Site\":\n",
        "          df.columns.values[column_index] = str(df.columns.values[column_index]) + \" \" + str(df.iloc[row_index+1, column_index])\n",
        "        break\n",
        "  # Filter the dataframe and for any cells that say no data or M replace that with a null value\n",
        "  # Hypothetically this should actually be something to check if columns are numeric\n",
        "  df = df.replace(\"no data\", pd.NA)\n",
        "\n",
        "  # Find the empty column dividing the two datasets and use it to break the dataframe into two seperate dataframes\n",
        "  for column_index in range(df.shape[1]):\n",
        "    if df.iloc[:, column_index].isnull().all():\n",
        "      new_df = df.iloc[:, column_index+1:].reset_index(drop=True)\n",
        "      df = df.iloc[:, :column_index].reset_index(drop=True)\n",
        "      break\n",
        "  # Use the Location column in df to make a new column called Location_note which is anything after Inlake or Inlet, then remove any characters after the words Inlake or Inlet in the Location column\n",
        "  df[\"Location\"] = df[\"Location\"].str.replace(\"inlet\", \"Inlet\")\n",
        "  df[\"Location_note\"] = df[\"Location\"].str.replace(\"Inlake\", \"\").str.replace(\"Inlet\", \"\").str.replace(\" \", \"\").str.replace(\"Outlet\", \"\")\n",
        "  df[\"Location\"] = df[\"Location\"].astype(str).apply(lambda x: \"Inlake\" if \"Inlake\" in x else x)\n",
        "  df[\"Location\"] = df[\"Location\"].astype(str).apply(lambda x: \"Inlet\" if \"Inlet\" in x else x)\n",
        "  df[\"Location\"] = df[\"Location\"].astype(str).apply(lambda x: \"Outlet\" if \"Outlet\" in x else x)\n",
        "  new_df[\"Location\"] = new_df[\"Location\"].str.replace(\"inlet\", \"Inlet\")\n",
        "  new_df[\"Location_note\"] = new_df[\"Location\"].str.replace(\"Inlake\", \"\").str.replace(\"Inlet\", \"\").str.replace(\" \", \"\").str.replace(\"Outlet\", \"\")\n",
        "  new_df[\"Location\"] = new_df[\"Location\"].astype(str).apply(lambda x: \"Inlake\" if \"Inlake\" in x else x)\n",
        "  new_df[\"Location\"] = new_df[\"Location\"].astype(str).apply(lambda x: \"Inlet\" if \"Inlet\" in x else x)\n",
        "  new_df[\"Location\"] = new_df[\"Location\"].astype(str).apply(lambda x: \"Outlet\" if \"Outlet\" in x else x)\n",
        "\n",
        "  # Remove unnecessary columns from the dataframes\n",
        "  # Iterate through the columns and find where there is only one value in the column or no values and drop those columns\n",
        "  df_drop = []\n",
        "  new_df_drop = []\n",
        "  for column_index in range(df.shape[1]):\n",
        "      if df.iloc[:, column_index].isnull().all() or df.iloc[:, column_index].notnull().sum() == 1:\n",
        "          df_drop.append(column_index)\n",
        "  for column_index in range(new_df.shape[1]):\n",
        "      if new_df.iloc[:, column_index].isnull().all() or new_df.iloc[:, column_index].notnull().sum() == 1:\n",
        "          new_df_drop.append(column_index)\n",
        "  # Drop the unnecessary columns\n",
        "  df = df.drop(df.columns[df_drop], axis=1)\n",
        "  new_df = new_df.drop(new_df.columns[new_df_drop], axis=1)\n",
        "\n",
        "  # Remove all unecessary rows from the dataframes using the site column\n",
        "  df = df[df[\"Site\"].notna() & (df[\"Site\"] != \"Site\")].reset_index(drop=True)\n",
        "  new_df = new_df[new_df[\"Site\"].notna() & (new_df[\"Site\"] != \"Site\")].reset_index(drop=True)\n",
        "\n",
        "  # Take the Year and Month columns and make a new column called Date which has both of them in a datetime format\n",
        "  df = df[df[\"Month\"].notna() & (df[\"Month\"] != \"no data\")].reset_index(drop=True)\n",
        "  new_df = new_df[new_df[\"Month\"].notna() & (new_df[\"Month\"] != \"no data\")].reset_index(drop=True)\n",
        "  df[\"Date\"] = pd.to_datetime(df[\"Year\"].astype(str) + \"-\" + df[\"Month\"].astype(str), format=\"%Y-%b\")\n",
        "  new_df[\"Date\"] = pd.to_datetime(new_df[\"Year\"].astype(str) + \"-\" + new_df[\"Month\"].astype(str), format=\"%Y-%b\")\n",
        "\n",
        "  # Add new columns\n",
        "  df[\"df_name\"] = sheet_name\n",
        "  df[\"Side\"] = 1\n",
        "  new_df[\"df_name\"] = sheet_name\n",
        "  new_df[\"Side\"] = 2\n",
        "\n",
        "  # Add new_df to the bottom of df\n",
        "  df = pd.concat([df, new_df], ignore_index=True)\n",
        "  dictionary[sheet_name] = df\n",
        "\n",
        "# Combine the dataframes and output a csv of the data\n",
        "combine = pd.concat(dictionary.values(), ignore_index=True)\n",
        "combine[\"National Park\"] = national_park\n",
        "combine.to_csv(output_name, index=False)\n",
        "print(f\"Data saved to the Colab file explorer as {output_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihcoSdIRgJuR",
        "outputId": "b0656a55-46ac-459a-efd6-8e294d5d8249"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to the Colab file explorer as YellowstoneLakesData-cleaned.csv\n"
          ]
        }
      ]
    }
  ]
}